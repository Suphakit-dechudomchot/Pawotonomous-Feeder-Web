// server.js - Backend Proxy à¸ªà¸³à¸«à¸£à¸±à¸š Gemini AI (Node.js + Express)
// à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡: npm install express cors dotenv node-fetch
import express from 'express';
import cors from 'cors';
import dotenv from 'dotenv';
import fetch from 'node-fetch';

dotenv.config();

const app = express();
const PORT = process.env.PORT || 3000;

app.use(cors());
app.use(express.json());
app.use(express.static('.')); // Serve static files

// Rate limiting
const requestCounts = new Map();
const RATE_LIMIT = 10; // 10 requests
const RATE_WINDOW = 60000; // per minute

function checkRateLimit(ip) {
    const now = Date.now();
    const userRequests = requestCounts.get(ip) || [];
    const recentRequests = userRequests.filter(time => now - time < RATE_WINDOW);
    
    if (recentRequests.length >= RATE_LIMIT) {
        return false;
    }
    
    recentRequests.push(now);
    requestCounts.set(ip, recentRequests);
    return true;
}

// Proxy endpoint à¸ªà¸³à¸«à¸£à¸±à¸š Gemini AI
app.post('/api/chat', async (req, res) => {
    const clientIp = req.ip;
    
    // Check rate limit
    if (!checkRateLimit(clientIp)) {
        return res.status(429).json({ 
            error: 'Too many requests. Please try again later.' 
        });
    }
    
    try {
        const { message } = req.body;
        
        if (!message || message.trim().length === 0) {
            return res.status(400).json({ error: 'Message is required' });
        }
        
        // Input validation
        if (message.length > 2000) {
            return res.status(400).json({ error: 'Message too long' });
        }
        
        const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
        
        if (!GEMINI_API_KEY) {
            throw new Error('Gemini API key not configured');
        }
        
        // Get available models
        const listUrl = `https://generativelanguage.googleapis.com/v1beta/models?key=${GEMINI_API_KEY}`;
        const listRes = await fetch(listUrl);
        const listData = await listRes.json();
        
        if (!listData.models || listData.models.length === 0) {
            throw new Error('No AI models available');
        }
        
        const chatModels = listData.models.filter(m => 
            m.supportedGenerationMethods.includes('generateContent')
        );
        
        // Try models until one works
        for (const model of chatModels) {
            try {
                const url = `https://generativelanguage.googleapis.com/v1beta/${model.name}:generateContent?key=${GEMINI_API_KEY}`;
                
                const response = await fetch(url, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        contents: [{ 
                            parts: [{ 
                                text: `à¸„à¸¸à¸“à¸„à¸·à¸­à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢ AI à¸‚à¸­à¸‡ Pawtonomous Feeder - à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¹ƒà¸«à¹‰à¸­à¸²à¸«à¸²à¸£à¸ªà¸±à¸•à¸§à¹Œà¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¸­à¸±à¸ˆà¸‰à¸£à¸´à¸¢à¸°

ðŸŽ¯ à¸„à¸§à¸²à¸¡à¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¸«à¸¥à¸±à¸:
1. à¸­à¸˜à¸´à¸šà¸²à¸¢à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡ Pawtonomous Feeder
2. à¹à¸™à¸°à¸™à¸³à¸à¸²à¸£à¹€à¸¥à¸µà¹‰à¸¢à¸‡à¸ªà¸±à¸•à¸§à¹Œà¹à¸¥à¸°à¸”à¸¹à¹à¸¥à¸ªà¸¸à¸‚à¸ à¸²à¸žà¸ªà¸³à¸«à¸£à¸±à¸šà¸ªà¸±à¸•à¸§à¹Œ: à¸«à¸¡à¸² à¹à¸¡à¸§ à¸à¸£à¸°à¸•à¹ˆà¸²à¸¢ à¸«à¸™à¸¹ à¸™à¸ à¸›à¸¥à¸²
3. à¸„à¸³à¹à¸™à¸°à¸™à¸³à¸Ÿà¸²à¸£à¹Œà¸¡à¹à¸¥à¸°à¹€à¸à¸©à¸•à¸£à¸à¸£ (à¸à¸²à¸£à¹€à¸¥à¸µà¹‰à¸¢à¸‡à¸ªà¸±à¸•à¸§à¹Œà¹€à¸Šà¸´à¸‡à¸žà¸²à¸“à¸´à¸Šà¸¢à¹Œ)

ðŸ“± à¸Ÿà¸µà¹€à¸ˆà¸­à¸£à¹Œà¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡ Pawtonomous:
- à¸•à¸±à¹‰à¸‡à¹€à¸§à¸¥à¸²à¹ƒà¸«à¹‰à¸­à¸²à¸«à¸²à¸£à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
- à¹ƒà¸«à¹‰à¸­à¸²à¸«à¸²à¸£à¸—à¸±à¸™à¸—à¸µ
- à¸£à¸°à¸šà¸šà¹€à¸›à¹ˆà¸²à¸¥à¸¡à¸­à¸±à¸ˆà¸‰à¸£à¸´à¸¢à¸°
- à¹€à¸‹à¹‡à¸™à¹€à¸‹à¸­à¸£à¹Œà¸§à¸±à¸”à¸£à¸°à¸”à¸±à¸šà¸­à¸²à¸«à¸²à¸£
- à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸à¸²à¸£à¹€à¸„à¸¥à¸·à¹ˆà¸­à¸™à¹„à¸«à¸§à¸ªà¸±à¸•à¸§à¹Œ
- à¹€à¸ªà¸µà¸¢à¸‡à¹€à¸£à¸µà¸¢à¸à¸ªà¸±à¸•à¸§à¹Œ 5 à¹€à¸ªà¸µà¸¢à¸‡

à¸•à¸­à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢ à¸à¸£à¸°à¸Šà¸±à¸š à¹€à¸›à¹‡à¸™à¸¡à¸´à¸•à¸£ à¹ƒà¸Šà¹‰ emoji

à¸„à¸³à¸–à¸²à¸¡: ${message}` 
                            }] 
                        }]
                    })
                });
                
                const data = await response.json();
                
                if (response.ok && data.candidates && data.candidates[0]) {
                    return res.json({ 
                        response: data.candidates[0].content.parts[0].text 
                    });
                } else if (response.status === 429) {
                    continue;
                }
            } catch (err) {
                continue;
            }
        }
        
        throw new Error('All AI models are currently unavailable');
        
    } catch (error) {
        console.error('API Error:', error);
        res.status(500).json({ 
            error: 'Failed to process request',
            message: error.message 
        });
    }
});

app.listen(PORT, () => {
    console.log(`ðŸš€ Server running on http://localhost:${PORT}`);
    console.log(`ðŸ”’ Secure proxy enabled for Gemini AI`);
});
